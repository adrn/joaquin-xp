{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141e3a6d",
   "metadata": {},
   "source": [
    "# Experiments related to Joaquin\n",
    "Technically, this notebook implements something *even dumber* than *Joaquin*.\n",
    "It implements kNN in *Gaia*-only quantities to get weighted-mean and weighted-least squares estimates of schmag or schmarrn.\n",
    "\n",
    "## Authors:\n",
    "- **Adrian Price-Whelan** (Flatiron)\n",
    "- **David W. Hogg** (NYU) (MPIA) (Flatiron)\n",
    "\n",
    "## Hyper-parameters:\n",
    "- `ncoeff`: The maximum number of BP and RP spectral coefficients to use in the project.\n",
    "- `pee_tree`: The number of features to use in the kdtree.\n",
    "- `maxk`: The maximum `k` to which we take neighbors; various `k` values are attempted.\n",
    "- `pee`: The number of features to use in the WLS and IRWLS.\n",
    "- scalings or preprocessing of input features (currently just normalization by `RP[0]`).\n",
    "- how we use the neighbors (weighted mean, weighted linear fit, mixture of some kind?).\n",
    "\n",
    "## To-do items and bugs:\n",
    "- We currently take ALL neighbors. But we don't need to consider neighbors that have obviously discrepant schmags given the extant Gaia data. Should we cut on schmag? Maybe?? It's complicated.\n",
    "- These models are effectively *discriminative*. We should make *generative* versions for science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f708a",
   "metadata": {},
   "source": [
    "## Read in and munge all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import astropy.table as at\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27342cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"./\"\n",
    "xm = at.Table.read(datadir + 'allStar-dr17-synspec-gaiadr3.fits')\n",
    "xm2 = at.Table.read(datadir + 'allStar-dr17-synspec-gaiadr3-gaiasourcelite.fits')\n",
    "xm2.rename_column('source_id', 'GAIADR3_SOURCE_ID')\n",
    "allstar = at.Table.read(datadir + 'allStarLite-dr17-synspec_rev1.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f49147",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = at.unique(at.hstack((allstar, xm)), keys='APOGEE_ID')\n",
    "tbl = tbl[tbl['GAIADR3_SOURCE_ID'] != 0]\n",
    "tbl = at.join(tbl, xm2, keys='GAIADR3_SOURCE_ID')\n",
    "len(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451885dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_xp_cont_filename = pathlib.Path(datadir + 'apogee-dr17-xpcontinuous.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and lightly rearrange\n",
    "xp_tbl = at.Table()\n",
    "with h5py.File(apogee_xp_cont_filename, 'r') as f:\n",
    "    xp_tbl['GAIADR3_SOURCE_ID'] = f['source_id'][:]\n",
    "    xp_tbl['bp'] = f['bp_coefficients'][:]\n",
    "    xp_tbl['rp'] = f['rp_coefficients'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07051f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and make simple cuts\n",
    "# Hogg: Why these cuts? MAGIC!\n",
    "xp_apogee_tbl = at.join(tbl, xp_tbl, keys='GAIADR3_SOURCE_ID')\n",
    "xp_apogee_tbl = xp_apogee_tbl[\n",
    "    (xp_apogee_tbl['TEFF'] > 3500.) &\n",
    "    (xp_apogee_tbl['TEFF'] < 6000.) &\n",
    "    (xp_apogee_tbl['LOGG'] > -0.5) &\n",
    "    (xp_apogee_tbl['LOGG'] < 5.5) &\n",
    "    (xp_apogee_tbl['M_H'] > -2.) &\n",
    "    (xp_apogee_tbl['AK_WISE'] < 3.)\n",
    "]\n",
    "len(xp_apogee_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70059f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xp_apogee_tbl.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c0190",
   "metadata": {},
   "source": [
    "## Make rectangular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does something useful!\n",
    "xp_apogee_tbl = xp_apogee_tbl.filled()\n",
    "print(len(xp_apogee_tbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make rectangular block of Gaia-only features (X) for training and validation\n",
    "# Note the gymnastics around normalizing by RP[0].\n",
    "\n",
    "# APW, HOGG: BUG: Why these cuts?\n",
    "feature_mask = (\n",
    "    (xp_apogee_tbl['J'] < 13) &\n",
    "    (xp_apogee_tbl['H'] < 12) &\n",
    "    (xp_apogee_tbl['K'] < 11)\n",
    ")\n",
    "\n",
    "ncoeff = 50 # MAGIC\n",
    "features = np.hstack((\n",
    "    (xp_apogee_tbl['phot_bp_mean_mag'] - xp_apogee_tbl['phot_rp_mean_mag'])[feature_mask, None],\n",
    "    (xp_apogee_tbl['bp'][:, 0:ncoeff] / xp_apogee_tbl['rp'][:, 0:1])[feature_mask],\n",
    "    (xp_apogee_tbl['rp'][:, 1:ncoeff + 1] / xp_apogee_tbl['rp'][:, 0:1])[feature_mask],\n",
    "))\n",
    "\n",
    "feature_names = np.concatenate((\n",
    "    ['$BP-RP$ (mag)', ],\n",
    "    [f'BP[{i}] / RP[0]' for i in range(0, ncoeff)],\n",
    "    [f'RP[{i}] / RP[0]' for i in range(1, ncoeff + 1)],\n",
    "))\n",
    "\n",
    "print(features.shape)\n",
    "print(len(feature_names), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange feature order because Hogg has issues\n",
    "index = np.concatenate((\n",
    "    [0, ], \n",
    "    *([i, ncoeff + i, ] for i in range(1, ncoeff + 1))\n",
    "))\n",
    "print(feature_names[index])\n",
    "features = features[:, index]\n",
    "feature_names = feature_names[index]\n",
    "print(features.shape, feature_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of labels (and label weights), aligned with the features.\n",
    "\n",
    "# Divide by 100 mas to get into correct maggie units.\n",
    "schmag_factor = 10 ** (0.2 * xp_apogee_tbl['phot_g_mean_mag'].value) / 100.\n",
    "\n",
    "labels = (xp_apogee_tbl['parallax'].value * schmag_factor)[feature_mask]\n",
    "print(labels.shape)\n",
    "\n",
    "label_errors = (xp_apogee_tbl['parallax_error'].value * schmag_factor)[feature_mask]\n",
    "print(label_errors.shape)\n",
    "\n",
    "label_weights = 1. / (label_errors ** 2)\n",
    "print(label_weights.shape)\n",
    "\n",
    "label_name = '$G$-band schmag (absmgy$^{-1/2}$)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15470cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_label_names = ['TEFF', 'LOGG', 'M_H', 'AK_WISE']\n",
    "alt_labels = np.hstack([(xp_apogee_tbl[foo])[feature_mask, None] for foo in alt_label_names])\n",
    "print(alt_labels.shape, alt_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the labels aren't wack\n",
    "plt.scatter(labels, labels / label_errors, c=\"k\", s=1., alpha=0.05)\n",
    "plt.axhline(np.median(labels / label_errors), color=\"k\")\n",
    "plt.xlim(-10., 50.)\n",
    "plt.ylim(-10., 200.)\n",
    "plt.xlabel(label_name)\n",
    "plt.ylabel(\"label SNR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5aaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the labels make sense wrt the APOGEE labels?\n",
    "for key in alt_label_names:\n",
    "    ff = plt.figure()\n",
    "    plt.scatter(xp_apogee_tbl[key].value[feature_mask], labels, c=\"k\", s=1., alpha=0.05)\n",
    "    plt.ylim(-0.2, 2.)\n",
    "    plt.xlabel(\"APOGEE \" + key)\n",
    "    plt.ylabel(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c04b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the features aren't wack\n",
    "for i in range(min(16, features.shape[1])):\n",
    "    ff = plt.figure()\n",
    "    foo = np.percentile(features[:, i], [2.5, 97.5])\n",
    "    lo = 0.5 * (foo[1] + foo[0]) - (foo[1] - foo[0])\n",
    "    hi = 0.5 * (foo[1] + foo[0]) + (foo[1] - foo[0])\n",
    "    plt.scatter(features[:, i], labels, c=\"k\", s=1., alpha=0.05)\n",
    "    plt.xlim(lo, hi)\n",
    "    plt.ylim(-10., 50.)\n",
    "    plt.xlabel(feature_names[i])\n",
    "    plt.ylabel(label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51c41f",
   "metadata": {},
   "source": [
    "## Make training and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut everything into eighths #MAGIC\n",
    "# BUG: Should fix random state more sensibly than this.\n",
    "np.random.seed(17)\n",
    "rando = np.random.randint(8, size=len(features))\n",
    "train = rando != 0\n",
    "valid = ~train\n",
    "print(np.sum(train), np.sum(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: Cut down the validation set to just luminous giants because that's all I care about!!\n",
    "# HACK: Also cut down validation to good SNR on the labels -- note that this move is legit.\n",
    "valid = (~train) \\\n",
    "      & (xp_apogee_tbl['LOGG'].value < 2.2)[feature_mask] \\\n",
    "      & (labels * np.sqrt(label_weights) > 10.)\n",
    "print(np.sum(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = features[train], features[valid]\n",
    "Y_train, Y_valid = labels[train], labels[valid]\n",
    "altY_train, altY_valid = alt_labels[train], alt_labels[valid]\n",
    "W_train, W_valid = label_weights[train], label_weights[valid]\n",
    "print(X_train.shape, X_valid.shape)\n",
    "print(Y_train.shape, Y_valid.shape)\n",
    "print(W_train.shape, W_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07796f11",
   "metadata": {},
   "source": [
    "## Build a kNN model and validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possibly useful validation-set neighbors up-front.\n",
    "# We'll use them in various ways below.\n",
    "pee_tree = 64 # magic\n",
    "maxk = 2 ** 10 # magic\n",
    "tree = KDTree(X_train[:, :pee_tree], leaf_size=32) # magic\n",
    "dists, inds = tree.query(X_valid[:, :pee_tree], k=maxk)\n",
    "print(X_valid.shape, dists.shape, inds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few objects\n",
    "for jj in range(8):\n",
    "    ii = np.random.randint(len(Y_valid))\n",
    "    while Y_valid[ii] > 1.:\n",
    "        ii = np.random.randint(len(Y_valid))\n",
    "    ff = plt.figure()\n",
    "    plt.axhline(Y_valid[ii], c=\"r\")\n",
    "    plt.errorbar(dists[ii], Y_train[inds[ii]], yerr = 1. / np.sqrt(W_train[inds[ii]]),\n",
    "                 fmt=\"o\", color=\"k\", ecolor=\"k\")\n",
    "    plt.xlabel(\"distance to neighbor\")\n",
    "    plt.ylabel(\"label (schmag) of neighbor\")\n",
    "    plt.title(f\"validation-set object {ii}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weighted mean.\n",
    "# BUG: this is a bad idea!\n",
    "ks = 2 ** np.arange(6)\n",
    "Y_hat_mean, Y_hat_mean_ivar = {}, {}\n",
    "for k in ks:\n",
    "    I = inds[:, :k]\n",
    "    Y_hat_mean_ivar[k] = np.sum(W_train[I], axis=1)\n",
    "    Y_hat_mean[k] = np.sum(W_train[I] * Y_train[I], axis=1) / Y_hat_mean_ivar[k]\n",
    "    print(k, Y_hat_mean[k].shape, Y_hat_mean_ivar[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ced68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma(ys, ys_true):\n",
    "    xs = (ys - ys_true) / (ys_true)\n",
    "    I = (ys_true > 0.4) # MAGIC\n",
    "    foo = np.percentile(xs[I], [16, 84])\n",
    "    return 0.5 * (foo[1] - foo[0])\n",
    "\n",
    "def plot_yhats(Y_hats, name):\n",
    "    for k in Y_hats.keys():\n",
    "        ff = plt.figure()\n",
    "        plt.plot([-100, 100], [-100, 100], \"k-\")\n",
    "        plt.plot(Y_valid, Y_hats[k], \"k.\", alpha=0.1)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.xlim(-0.2, 2)\n",
    "        plt.ylim(-0.2, 2)\n",
    "        plt.xlabel(\"Gaia-measured schmag\")\n",
    "        plt.ylabel(name + \" predicted schmag\")\n",
    "        sigma = get_sigma(Y_hats[k], Y_valid)\n",
    "        plt.title(name + \", $k={0}$, fractional $\\sigma={1:4.2f}$\".format(k, sigma))\n",
    "\n",
    "plot_yhats(Y_hat_mean, \"mean of KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test unweighted median\n",
    "# BUG: this is a bad idea!\n",
    "Y_hat_med = {}\n",
    "for k in ks:\n",
    "    I = inds[:, :k]\n",
    "    Y_hat_med[k] = np.median(Y_train[I], axis=1)\n",
    "    print(k, Y_hat_med[k].shape, get_sigma(Y_hat_med[k], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yhats(Y_hat_med, \"median of KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a linear weighted least squares as a function of k\n",
    "# BUG: UNTESTED\n",
    "# BUG: DOESN'T RETURN IVARS\n",
    "ks = 2 ** np.arange(5, 11)\n",
    "pee = 16\n",
    "Y_hat_wls, Y_hat_wls_ivar = {}, {}\n",
    "for k in ks:\n",
    "    Y_hat_wls[k] = np.zeros_like(Y_valid) + np.NaN\n",
    "    I = inds[:, :k]\n",
    "    for i, II in tqdm(enumerate(I)):\n",
    "        # make design matrix\n",
    "        X = np.hstack((np.ones((k, 1)), X_train[II, :pee - 1]))\n",
    "        Xstar = np.append(1, X_valid[i, :pee - 1])\n",
    "        Cinv = W_train[II]\n",
    "        Y = Y_train[II]\n",
    "        Y_hat_wls[k][i] = Xstar @ np.linalg.lstsq(X.T @ (Cinv[:, None] * X),\n",
    "                                                  X.T @ (Cinv * Y),\n",
    "                                                  rcond=None)[0]\n",
    "    print(k, pee, Y_hat_wls[k].shape, get_sigma(Y_hat_wls[k], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e41e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yhats(Y_hat_wls, \"WLS ($p={}$) of KNN\".format(pee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af396567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test an IRWLS version\n",
    "def irwls(X, Y, ivar, Q=25., niter=3, rcond=None):\n",
    "    \"\"\"\n",
    "    # Iteratively reweighted least squares\n",
    "    Supposed to be like `np.linalg.solve()` but better maybe?\n",
    "    \n",
    "    ## Inputs:\n",
    "    - `X`: shape `(n, p)` design matrix or features\n",
    "    - `Y`: shape `(n, )` labels or data\n",
    "    - `ivar`: shape `(n, )` input data weights (inverse variances perhaps?)\n",
    "    - `Q`: control parameter for IRWLS (like `nsigma ** 2`; should be larger than unity)\n",
    "    - `niter`: number of iterations to perform\n",
    "    - `rcond`: input to `np.linalg.lstsq()`\n",
    "    \n",
    "    ## Outputs:\n",
    "    - `beta`: shape `(p, )` vector of linear coefficients\n",
    "    \n",
    "    ## Bugs:\n",
    "    - Should maybe test for convergence rather than just `niter`?\n",
    "    - Should return inverse variance tensor as well?\n",
    "    \"\"\"\n",
    "    beta = np.linalg.lstsq(X.T @ (ivar[:, None] * X), X.T @ (ivar * Y),\n",
    "                           rcond=rcond)[0]\n",
    "    for i in range(niter):\n",
    "        chi2 = ivar * (Y - X @ beta) ** 2\n",
    "        W = ivar * (1. / (1. + chi2 / Q))\n",
    "        beta = np.linalg.lstsq(X.T @ (W[:, None] * X), X.T @ (W * Y),\n",
    "                           rcond=rcond)[0]\n",
    "    return beta\n",
    "\n",
    "ks = 2 ** np.arange(5, 10)\n",
    "pee = 24\n",
    "QQ = 25.\n",
    "Y_hat_irwls, Y_hat_irwls_ivar = {}, {}\n",
    "for k in ks:\n",
    "    Y_hat_irwls[k] = np.zeros_like(Y_valid) + np.NaN\n",
    "    Y_hat_irwls_ivar[k] = np.zeros_like(Y_valid) + np.NaN\n",
    "    I = inds[:, :k]\n",
    "    for i, II in tqdm(enumerate(I)):\n",
    "        # make design matrix\n",
    "        X = np.hstack((np.ones((k, 1)), X_train[II, :pee - 1]))\n",
    "        Xstar = np.append(1, X_valid[i, :pee - 1])\n",
    "        Y_hat_irwls[k][i] = Xstar @ irwls(X, Y_train[II], W_train[II], Q=QQ)\n",
    "    print(k, pee, QQ, Y_hat_irwls[k].shape, get_sigma(Y_hat_irwls[k], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bfa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yhats(Y_hat_irwls, \"IRWLS ($p={0}$, $Q={1:.0f}$) of KNN\".format(pee, QQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da6021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the cheat mode -- infer labels from alt_labels for the same set of stars\n",
    "ks = 2 ** np.arange(4, 9)\n",
    "pee = len(altY_train[0]) + 1\n",
    "QQ = 25.\n",
    "Y_hat_cheat_irwls, Y_hat_cheat_irwls_ivar = {}, {}\n",
    "for k in ks:\n",
    "    Y_hat_cheat_irwls[k] = np.zeros_like(Y_valid) + np.NaN\n",
    "    Y_hat_cheat_irwls_ivar[k] = np.zeros_like(Y_valid) + np.NaN\n",
    "    I = inds[:, :k]\n",
    "    for i, II in tqdm(enumerate(I)):\n",
    "        # make design matrix\n",
    "        X = np.hstack((np.ones((k, 1)), altY_train[II]))\n",
    "        Xstar = np.append(1, altY_valid[i])\n",
    "        Y_hat_cheat_irwls[k][i] = Xstar @ irwls(X, Y_train[II], W_train[II], Q=QQ)\n",
    "    print(k, pee, QQ, Y_hat_cheat_irwls[k].shape, get_sigma(Y_hat_cheat_irwls[k], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yhats(Y_hat_cheat_irwls, \"cheating ($p={0}$, $Q={1:.0f}$) with KNN\".format(pee, QQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e806cf1",
   "metadata": {},
   "source": [
    "## Run this model on EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ea9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APW: We need to figure out the above tests and then run in the data center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa9cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
