{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b6ea29",
   "metadata": {},
   "source": [
    "# Schlummernd: Linear Latent Variable Model\n",
    "See the Text.\n",
    "\n",
    "## TODO / questions\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53898a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import corner\n",
    "import astropy.coordinates as coord\n",
    "from astropy.stats import median_absolute_deviation as MAD\n",
    "import astropy.table as at\n",
    "import astropy.units as u\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KDTree\n",
    "from pyia import GaiaData\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import jaxopt\n",
    "import optax\n",
    "\n",
    "from schlummernd import LinearLVM\n",
    "from schlummernd.data import load_data, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437b0d",
   "metadata": {},
   "source": [
    "# Load APOGEE x Gaia data\n",
    "\n",
    "see `Assemble-data.ipynb` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec653859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper giant branch:\n",
    "# g = load_data(\n",
    "#     filters=dict(\n",
    "#         TEFF=(3000, 5100), \n",
    "#         LOGG=(-0.5, 2.3),\n",
    "#         M_H=(-3, None),\n",
    "#         phot_g_mean_mag=(None, 15.5*u.mag),\n",
    "#         AK_WISE=(-0.1, None)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# For red clump instead:\n",
    "g_all = load_data(\n",
    "    filters=dict(\n",
    "        TEFF=(4500, 5100), \n",
    "        LOGG=(2.3, 2.6),\n",
    "        M_H=(-3, None),\n",
    "        phot_g_mean_mag=(None, 15.*u.mag),\n",
    "        AK_WISE=(-0.1, None),\n",
    "        # HACK:\n",
    "        TEFF_ERR=(0, 75),\n",
    "        M_H_ERR=(0, 0.05),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g_all[(np.abs(g_all.b) > 15*u.deg) & (g_all.SFD_EBV < 0.2)]\n",
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02123e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprp = (g.phot_bp_mean_mag - g.phot_rp_mean_mag).value\n",
    "mg = (g.phot_g_mean_mag - g.get_distance(allow_negative=True).distmod).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "H, xb, yb, _ = ax.hist2d(\n",
    "    g.TEFF,\n",
    "    g.LOGG,\n",
    "    bins=(\n",
    "        np.linspace(3000, 8000, 128),\n",
    "        np.linspace(-0.5, 5.5, 128)\n",
    "    ),\n",
    "    norm=mpl.colors.LogNorm()\n",
    ")\n",
    "ax.set_xlim(xb.max(), xb.min())\n",
    "ax.set_ylim(yb.max(), yb.min())\n",
    "ax.set_xlabel('TEFF')\n",
    "ax.set_ylabel('LOGG')\n",
    "\n",
    "ax = axes[1]\n",
    "H, xb, yb, _ = ax.hist2d(\n",
    "    bprp,\n",
    "    mg,\n",
    "    bins=(\n",
    "        np.linspace(-0.5, 3, 128),\n",
    "        np.linspace(-4, 10.5, 128)\n",
    "    ),\n",
    "    norm=mpl.colors.LogNorm()\n",
    ")\n",
    "ax.set_xlim(xb.min(), xb.max())\n",
    "ax.set_ylim(yb.max(), yb.min())\n",
    "ax.set_xlabel('BP-RP')\n",
    "ax.set_ylabel('$M_G$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45c675",
   "metadata": {},
   "source": [
    "# Construct features and labels\n",
    "\n",
    "Make list of possible labels (and label weights), aligned with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551eef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_features = {\n",
    "#     r\"$G_{\\rm BP}-G_{\\rm RP}$\": 0.1 * (g.phot_bp_mean_mag - g.phot_rp_mean_mag)\n",
    "# }\n",
    "# f_all = Features.from_gaiadata(g, n_bp=32, n_rp=32) # , **other_features)\n",
    "f_all = Features.from_gaiadata(g, n_bp=5, n_rp=5) # , **other_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of labels (and label weights), aligned with the features.\n",
    "\n",
    "label_ys = {}\n",
    "label_errs = {}\n",
    "label_latex = {}\n",
    "label_y_perc = {}\n",
    "\n",
    "schmag_factor = 10 ** (0.2 * g.phot_g_mean_mag.value) / 100.\n",
    "schmag_err = g.parallax_error.value * schmag_factor\n",
    "# label_ys['schmag'] = g.parallax.value * schmag_factor\n",
    "# label_errs['schmag'] = schmag_err\n",
    "# label_latex['schmag'] = '$G$-band schmag (absmgy$^{-1/2}$)'\n",
    "\n",
    "for name in ['TEFF', 'M_H', 'LOGG']: # , 'AK_WISE']:\n",
    "    label_y_perc[name] = np.nanpercentile(g[name], [16, 50, 84])\n",
    "    scale = (label_y_perc[name][2] - label_y_perc[name][0])\n",
    "    \n",
    "    err_col = f'{name}_ERR'\n",
    "    label_ys[name] = (g[name] - label_y_perc[name][1]) / scale\n",
    "    if err_col in g.data.colnames:\n",
    "        label_errs[name] = g[err_col]\n",
    "    elif name == 'AK_WISE':\n",
    "        label_errs[name] = 0.05 * np.ones_like(label_ys[name])\n",
    "    label_errs[name] = label_errs[name] / scale\n",
    "\n",
    "label_latex['M_H'] = r\"$[{\\rm M}/{\\rm H}]$\"\n",
    "label_latex['LOGG'] = r\"$\\log g$\"\n",
    "label_latex['TEFF'] = r\"$T_{\\rm eff}$\"\n",
    "label_latex['AK_WISE'] = r\"$A_K$\"\n",
    "\n",
    "label_y = np.hstack([np.array(x)[:, None] for x in label_ys.values()])\n",
    "label_err = np.hstack([np.array(x)[:, None] for x in label_errs.values()])\n",
    "assert np.all(label_err > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untransform_y(y, meta):\n",
    "    new_y = {}\n",
    "    for i, name in enumerate(label_ys.keys()):\n",
    "        scale = (meta[name][2] - meta[name][0])\n",
    "        new_y[name] = y[:, i] * scale + meta[name][1]\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNN = 10\n",
    "# plt.errorbar(\n",
    "#     g.M_H[:NNN], \n",
    "#     label_ys['M_H'][:NNN],\n",
    "#     xerr=g.M_H_ERR[:NNN],\n",
    "#     yerr=label_errs['M_H'][:NNN],\n",
    "#     ls='none',\n",
    "#     marker='o'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de21c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_y.shape, label_err.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corner.corner(label_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99758742",
   "metadata": {},
   "source": [
    "# Make training and validation samples\n",
    "\n",
    "cut into eighths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5941dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "rando = rng.integers(8, size=len(f_all))\n",
    "train = rando != 0\n",
    "valid = (\n",
    "    ~train #&\n",
    "#     (g.LOGG < 2.2) &\n",
    "#     ((label_ys[label_name] * np.sqrt(label_weights[label_name])) > 4)\n",
    ")\n",
    "\n",
    "f_train = f_all[train]\n",
    "f_valid = f_all[valid]\n",
    "\n",
    "X_train, X_valid = f_train.X, f_valid.X\n",
    "X_train_err, X_valid_err = f_train.X_err, f_valid.X_err\n",
    "y_train, y_valid = label_y[train], label_y[valid]\n",
    "y_train_err, y_valid_err = label_err[train], label_err[valid]\n",
    "\n",
    "print(X_train.shape, X_valid.shape)\n",
    "print(X_train_err.shape, X_valid_err.shape)\n",
    "print(y_train.shape, y_valid.shape)\n",
    "print(y_train_err.shape, y_valid_err.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = corner.corner(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2960f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = corner.corner(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ba69a",
   "metadata": {},
   "source": [
    "# Find neighbors for validation sample stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 128  # TODO: need to assess this  \n",
    "# n_xp_tree = 8  # MAGIC\n",
    "n_xp_tree = 5\n",
    "\n",
    "X_tree = f_all.slice_bp(n_xp_tree).slice_rp(n_xp_tree).X_tree\n",
    "X_train_tree = X_tree[train]\n",
    "X_valid_tree = X_tree[valid]\n",
    "\n",
    "tree = KDTree(X_train_tree, leaf_size=32) # magic\n",
    "dists, inds = tree.query(X_valid_tree, k=K)\n",
    "print(X_valid.shape, dists.shape, inds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0c95",
   "metadata": {},
   "source": [
    "# Run LLVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoggLLVM:\n",
    "    def __init__(self, X, Y, Wx, Wy, B, Lambda):\n",
    "        \"\"\"\n",
    "        ## inputs:\n",
    "        `X`: shape `(n, r)` block of training features\n",
    "        `Y`: shape `(n, q)` block of training labels\n",
    "        `Wx`: shape `(n, r)` block of inverse variances (weights) for the features\n",
    "        `Wy`: shape `(n, q)` block of inverse variances (weights) for the labels\n",
    "        `B`: shape `(q, d)` matrix translating latents to labels.\n",
    "        `Lambda`: regularization strength; use the source, Luke.\n",
    "        \n",
    "        ## bugs / comments:\n",
    "        - Put `np.NaN` into the parts of `B` you want to optimize!\n",
    "        - This code more-or-less assumes that your data and labels and `B` are\n",
    "          normalized to reasonable ranges.\n",
    "        - In the long run, `X` and `Y` don't need to be the same length.\n",
    "        \"\"\"\n",
    "        self.X = jnp.array(X)\n",
    "        self.Y = jnp.array(Y)\n",
    "        self.n, self.r = self.X.shape\n",
    "        enn, self.q = self.Y.shape\n",
    "        assert enn == self.n, \"llvm: Right now, the training data must be rectangular.\"\n",
    "        self.Wx = jnp.array(Wx)\n",
    "        assert self.Wx.shape == self.X.shape, \"llvm: Inconsistency between `X` and `Wx`.\"\n",
    "        assert np.all(self.Wx >= 0.), \"llvm: Weights can't be negative.\"\n",
    "        self.sqrtWx = jnp.sqrt(self.Wx)\n",
    "        self.Wy = jnp.array(Wy)\n",
    "        assert self.Wy.shape == self.Y.shape, \"llvm: Inconsistency between `Y` and `Wy`.\"\n",
    "        assert np.all(self.Wy >= 0.), \"llvm: Weights can't be negative.\"\n",
    "        self.sqrtWy = jnp.sqrt(Wy)\n",
    "        self.B = jnp.array(B)\n",
    "        cue, self.d = B.shape\n",
    "        assert cue == self.q, \"llvm: Inconsistency between `B` and `Y`.\"\n",
    "        self.B_elements_to_fit = jnp.isnan(self.B)\n",
    "        if jnp.sum(self.B_elements_to_fit) == 0:\n",
    "            self.B_elements_to_fit = None\n",
    "            print(\"llvm: found no free elements of `B`.\")\n",
    "        else:\n",
    "            self.B[self.B_elements_to_fit] = 0.\n",
    "            print(\"llvm: found\", np.sum(self.B_elements_to_fit),\n",
    "                  \"free elements of `B`.\")\n",
    "        self.free_elements_of_Z = jnp.isclose(jnp.sum(self.B, axis=0), 0.)\n",
    "        print(self.free_elements_of_Z)\n",
    "        print(\"llvm: found\", np.sum(self.free_elements_of_Z),\n",
    "              \"unconstrained elements of `Z`, of\", self.d)\n",
    "        self.Lambda = Lambda\n",
    "        self.regularization_matrix = Lambda * np.diag(self.free_elements_of_Z.astype(int))\n",
    "        print(self.regularization_matrix)\n",
    "        assert Lambda > 0., \"llvm: You must regularize, and strictly positively.\"\n",
    "        self.initialize_latents()\n",
    "        return\n",
    "\n",
    "    def _predict_X(self):\n",
    "        return self.mux[None, :] + self.Z @ self.A.T\n",
    "\n",
    "    def _predict_Y(self):\n",
    "        return self.muy[None, :] + self.Z @ self.B.T\n",
    "\n",
    "    def _chi_X(self):\n",
    "        return (self.X - self._predict_X()) * self.sqrtWx\n",
    "\n",
    "    def _chi_Y(self):\n",
    "        return (self.Y - self._predict_Y()) * self.sqrtWy\n",
    "\n",
    "    def _cost(self):\n",
    "        \"\"\"\n",
    "        WARNING: Regularization term is totally wrong.\n",
    "        \"\"\"\n",
    "        Xchi, Ychi = self._chi_X(), self._chi_Y()\n",
    "        return 0.5 * jnp.sum(self._chi_X() ** 2) \\\n",
    "             + 0.5 * jnp.sum(self._chi_Y() ** 2) \\\n",
    "             + 0.5 * self.Lambda * jnp.sum(self.Z[:, self.free_elements_of_Z] ** 2)\n",
    "\n",
    "    def predict_y_given_x(self, Xstar, Wxstar):\n",
    "        # should this use the regularization matrix? Hogg thinks not.\n",
    "        m, arrh = Xstar.shape\n",
    "        assert arrh == self.r\n",
    "        Ystarhat = np.zeros((m, self.q))\n",
    "        sqrtWx = np.sqrt(Wxstar)\n",
    "        chi = (Xstar - self.mux[None, :]) * sqrtWx\n",
    "        for i, x in enumerate(chi):\n",
    "            M = self.A * sqrtWx[i][:, None]\n",
    "            z = np.linalg.lstsq(M, x, rcond=None)[0]\n",
    "            Ystarhat[i] = self.muy + self.B @ z\n",
    "        return Ystarhat\n",
    "\n",
    "    def initialize_latents(self):\n",
    "        # this is a bag of hacks.\n",
    "\n",
    "        # Zeoth hack: Take means.\n",
    "        self.mux = jnp.sum(self.X * self.Wx, axis=0) / jnp.sum(self.Wx, axis=0)\n",
    "        assert self.mux.shape == (self.r, )\n",
    "        self.muy = jnp.sum(self.Y * self.Wy, axis=0) / jnp.sum(self.Wy, axis=0)\n",
    "        assert self.muy.shape == (self.q, )\n",
    "\n",
    "        # First hack: Start with the pseudo-inverse of `B`.\n",
    "        # BUG: Doesn't use weights.\n",
    "        self.Z = jnp.linalg.lstsq(self.B, (self.Y - self.muy[None, :]).T, rcond=None)[0].T\n",
    "        assert self.Z.shape == (self.n, self.d)\n",
    "\n",
    "        # Second hack: Add noise.\n",
    "        TINY = 1.e-1 # MAGIC\n",
    "        sigma = np.std(self.Z) + TINY\n",
    "        self.Z += 0.1 * sigma * np.random.normal(size=self.Z.shape) # MAGIC\n",
    "\n",
    "        # Third hack: Run A and B steps.\n",
    "        self.A = jnp.zeros((self.r, self.d))\n",
    "        self.A_step()\n",
    "        self.B_step()\n",
    "        \n",
    "    def optimize_step(self, ftol=0.1):\n",
    "        self._renorm_A()\n",
    "        before = self._cost()\n",
    "        self.Z_step()\n",
    "        self.mu_step()\n",
    "        self.A_step()\n",
    "        self.B_step()\n",
    "        after = self._cost()\n",
    "        print(\"optimize_step(): Cost after:\", after, self.n * (self.r + self.q))\n",
    "        return after < (before - ftol)\n",
    "\n",
    "    def Z_step(self):\n",
    "        # BUG: DOESN'T DO REGULARIZATION RIGHT.\n",
    "        dZ = np.zeros_like(self.Z)\n",
    "        for i, (x, y) in enumerate(zip(self._chi_X(),\n",
    "                                       self._chi_Y())):\n",
    "            resid = np.append(x, y)\n",
    "            matrix = np.concatenate((self.A * (self.sqrtWx[i])[:, None],\n",
    "                                     self.B * (self.sqrtWy[i])[:, None]),\n",
    "                                    axis=0)\n",
    "            dZ[i] = np.linalg.lstsq(matrix.T @ matrix\n",
    "                                    + self.regularization_matrix,\n",
    "                                    matrix.T @ resid, rcond=None)[0]\n",
    "        self.Z = self.Z + dZ\n",
    "        return\n",
    "\n",
    "    def A_step(self):\n",
    "        dA = np.zeros_like(self.A)\n",
    "        for j, x in enumerate(self._chi_X().T):\n",
    "            dA[j] = np.linalg.lstsq(self.Z * self.sqrtWx[:, j][None, :].T,\n",
    "                                    x, rcond=None)[0]\n",
    "        self.A = self.A + dA\n",
    "        return\n",
    "\n",
    "    def _renorm_A(self):\n",
    "        renorm = np.sqrt(np.sum(self.A[:, self.free_elements_of_Z] ** 2, axis=0))\n",
    "        self.A.at[:, self.free_elements_of_Z].divide(renorm[None, :])\n",
    "        self.Z.at[:, self.free_elements_of_Z].multiply(renorm[None, :])\n",
    "        return\n",
    "\n",
    "    def B_step(self):\n",
    "        # BUG: Not currently operable.\n",
    "        if self.B_elements_to_fit is None:\n",
    "            return\n",
    "        assert False\n",
    "        return\n",
    "\n",
    "    def mu_step(self):\n",
    "        Xresid = self.X - self._predict_X()\n",
    "        self.mux = self.mux + jnp.sum(Xresid * self.Wx, axis=0) / jnp.sum(self.Wx, axis=0)\n",
    "        Yresid = self.Y - self._predict_Y()\n",
    "        self.muy = self.muy + jnp.sum(Yresid * self.Wy, axis=0) / jnp.sum(self.Wy, axis=0)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90136ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_n = 20\n",
    "idx = inds[valid_n]\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "n_labels = y_train.shape[1]\n",
    "n_latents = n_labels + 3\n",
    "B = np.zeros((n_labels, n_latents))\n",
    "B[:n_labels, :n_labels] = np.eye(n_labels)\n",
    "\n",
    "llvm_hogg = HoggLLVM(\n",
    "    X_train[idx], y_train[idx], \n",
    "    1/X_train_err[idx]**2, 1/y_train_err[idx]**2, \n",
    "    B, 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46eb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "while llvm_hogg.optimize_step():\n",
    "    True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = llvm_hogg.predict_y_given_x(X_valid[valid_n:valid_n+1], \n",
    "                                   1/X_valid_err[valid_n:valid_n+1]**2)\n",
    "Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f532e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransform_y(Yhat, label_y_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransform_y(y_valid[valid_n:valid_n+1], label_y_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56620eba",
   "metadata": {},
   "source": [
    "## Per neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3fbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bceb3345",
   "metadata": {},
   "source": [
    "## Per-source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1db4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_n = 10\n",
    "idx = inds[valid_n]\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "n_labels = y_train.shape[1]\n",
    "n_latents = n_labels + 1\n",
    "B = np.zeros((n_labels, n_latents))\n",
    "B[:n_labels, :n_labels] = np.eye(n_labels)\n",
    "\n",
    "llvm = LinearLVM(\n",
    "    X_train[idx], y_train[idx], \n",
    "    X_train_err[idx], y_train_err[idx], \n",
    "    B, alpha=1e-3, beta=1., \n",
    "    verbose=True, rng=rng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = llvm.pack_p()\n",
    "print(x0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = jaxopt.LBFGS(fun=llvm, maxiter=2**15)\n",
    "res_bfgs = solver.run(x0)\n",
    "res_state = llvm.unpack_p(res_bfgs.params)\n",
    "print(res_bfgs.state.iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    llvm(x0),\n",
    "    llvm(res_bfgs.params)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe8c3a",
   "metadata": {},
   "source": [
    "### Self-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict0 = llvm.predict_y(\n",
    "    X_train[idx], \n",
    "    X_train_err[idx], \n",
    "    llvm.par_state\n",
    ")\n",
    "\n",
    "y_train_predict = llvm.predict_y(\n",
    "    X_train[idx], \n",
    "    X_train_err[idx],  \n",
    "    res_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range(y_train.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.scatter(\n",
    "        y_train[idx, q],\n",
    "        y_train_predict0[:, q]\n",
    "    )\n",
    "    plt.scatter(\n",
    "        y_train[idx, q],\n",
    "        y_train_predict[:, q]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = llvm.pack_p()\n",
    "print(x0.shape)\n",
    "llvm(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_predict0 = llvm.predict_y(\n",
    "    X_valid[valid_n:valid_n+1], \n",
    "    X_valid_err[valid_n:valid_n+1], \n",
    "    llvm.par_state\n",
    ")\n",
    "\n",
    "y_valid_predict = llvm.predict_y(\n",
    "    X_valid[valid_n:valid_n+1], \n",
    "    X_valid_err[valid_n:valid_n+1], \n",
    "    res_state\n",
    ")\n",
    "\n",
    "print(y_valid[valid_n:valid_n+1])\n",
    "print(y_valid_predict0)\n",
    "print(y_valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransform_y(y_valid[valid_n:valid_n+1], label_y_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "untransform_y(y_valid_predict, label_y_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccad915",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8067d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = jaxopt.LBFGS(fun=llvm, maxiter=10000)\n",
    "res_bfgs = solver.run(x0)\n",
    "print(res_bfgs.state.iter_num)\n",
    "llvm(res_bfgs.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.adam(1.)\n",
    "solver = jaxopt.OptaxSolver(opt=opt, fun=llvm, maxiter=100000)\n",
    "res_adam = solver.run(x0)\n",
    "print(res_adam.state.iter_num)\n",
    "llvm(res_adam.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llvm.unpack_p(res_bfgs.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_state = llvm.unpack_p(res_bfgs.params)\n",
    "# res_state = llvm.unpack_p(res_adam.params)\n",
    "\n",
    "# y_valid_predict0 = llvm.predict_y(X_valid, X_valid_err, llvm.par_state)\n",
    "# y_valid_predict = llvm.predict_y(X_valid, X_valid_err, res_state)\n",
    "\n",
    "y_valid_predict = llvm.predict_y(\n",
    "    X_valid[valid_n:valid_n+1], \n",
    "    X_valid_err[valid_n:valid_n+1], \n",
    "    res_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid[valid_n:valid_n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_predict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb635194",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78004528",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_err[valid_n:valid_n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer for test-set objects\n",
    "for k in range(y_valid.shape[1]):\n",
    "    plt.figure()\n",
    "#     plt.scatter(y_star[:, k], ystar_predict0[:, k], c=\"r\", marker=\"o\")\n",
    "    plt.scatter(y_valid[:, k], y_valid_predict[:, k], c=\"k\", marker=\"o\")\n",
    "    plt.plot([y_valid[:, k].min(), y_valid[:, k].max()],\n",
    "             [y_valid[:, k].min(), y_valid[:, k].max()], \n",
    "             marker='', color='tab:blue')\n",
    "    plt.xlabel(f\"true label {k}\")\n",
    "    plt.ylabel(f\"prediction of label {k}\")\n",
    "    plt.title(\"held-out data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961fccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrian conda base",
   "language": "python",
   "name": "conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
