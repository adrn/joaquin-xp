{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b6ea29",
   "metadata": {},
   "source": [
    "# Linear Latent Variable Model\n",
    "See the Text.\n",
    "\n",
    "## Authors:\n",
    "- **Adrian Price-Whelan** (Flatiron)\n",
    "- **David W. Hogg** (NYU) (MPIA) (Flatiron)\n",
    "\n",
    "## TODO / questions\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53898a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import astropy.coordinates as coord\n",
    "from astropy.stats import median_absolute_deviation as MAD\n",
    "import astropy.table as at\n",
    "import astropy.units as u\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KDTree\n",
    "from pyia import GaiaData\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from helpers import load_data, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437b0d",
   "metadata": {},
   "source": [
    "# Load APOGEE x Gaia data\n",
    "\n",
    "see `Assemble-data.ipynb` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec653859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper giant branch:\n",
    "# g = load_data(\n",
    "#     filters=dict(\n",
    "#         TEFF=(3000, 5100), \n",
    "#         LOGG=(-0.5, 2.3),\n",
    "#         M_H=(-3, None),\n",
    "#         phot_g_mean_mag=(None, 15.5*u.mag),\n",
    "#         AK_WISE=(-0.1, None)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# For red clump instead:\n",
    "g = load_data(\n",
    "    filters=dict(\n",
    "        TEFF=(4500, 5100), \n",
    "        LOGG=(2.3, 2.6),\n",
    "        M_H=(-3, None),\n",
    "        phot_g_mean_mag=(None, 15.5*u.mag),\n",
    "        AK_WISE=(-0.1, None)\n",
    "    )\n",
    ")\n",
    "\n",
    "g = g[(np.abs(g.b) > 15*u.deg) & (g.SFD_EBV < 0.2)]\n",
    "\n",
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02123e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprp = (g.phot_bp_mean_mag - g.phot_rp_mean_mag).value\n",
    "mg = (g.phot_g_mean_mag - g.get_distance(allow_negative=True).distmod).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "H, xb, yb, _ = ax.hist2d(\n",
    "    g.TEFF,\n",
    "    g.LOGG,\n",
    "    bins=(\n",
    "        np.linspace(3000, 8000, 128),\n",
    "        np.linspace(-0.5, 5.5, 128)\n",
    "    ),\n",
    "    norm=mpl.colors.LogNorm()\n",
    ")\n",
    "ax.set_xlim(xb.max(), xb.min())\n",
    "ax.set_ylim(yb.max(), yb.min())\n",
    "ax.set_xlabel('TEFF')\n",
    "ax.set_ylabel('LOGG')\n",
    "\n",
    "ax = axes[1]\n",
    "H, xb, yb, _ = ax.hist2d(\n",
    "    bprp,\n",
    "    mg,\n",
    "    bins=(\n",
    "        np.linspace(-0.5, 3, 128),\n",
    "        np.linspace(-4, 10.5, 128)\n",
    "    ),\n",
    "    norm=mpl.colors.LogNorm()\n",
    ")\n",
    "ax.set_xlim(xb.min(), xb.max())\n",
    "ax.set_ylim(yb.max(), yb.min())\n",
    "ax.set_xlabel('BP-RP')\n",
    "ax.set_ylabel('$M_G$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45c675",
   "metadata": {},
   "source": [
    "# Construct features and labels\n",
    "\n",
    "Make list of possible labels (and label weights), aligned with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551eef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features = {\n",
    "    r\"$G_{\\rm BP}-G_{\\rm RP}$\": 0.1 * (g.phot_bp_mean_mag - g.phot_rp_mean_mag)\n",
    "}\n",
    "f_all = Features.from_gaiadata(g, n_bp=32, n_rp=32, **other_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of labels (and label weights), aligned with the features.\n",
    "\n",
    "label_ys = {}\n",
    "label_errs = {}\n",
    "label_latex = {}\n",
    "\n",
    "schmag_factor = 10 ** (0.2 * g.phot_g_mean_mag.value) / 100.\n",
    "schmag_err = g.parallax_error.value * schmag_factor\n",
    "label_ys['schmag'] = g.parallax.value * schmag_factor\n",
    "label_errs['schmag'] = schmag_err\n",
    "label_latex['schmag'] = '$G$-band schmag (absmgy$^{-1/2}$)'\n",
    "\n",
    "for name in ['M_H', 'LOGG', 'TEFF']: # , 'AK_WISE']:\n",
    "    err_col = f'{name}_ERR'\n",
    "    label_ys[name] = g[name]\n",
    "    if err_col in g.data.colnames:\n",
    "        label_errs[name] = g[err_col]\n",
    "    elif name == 'AK_WISE':\n",
    "        label_errs[name] = 0.05 * np.ones_like(label_ys[name])\n",
    "\n",
    "label_latex['M_H'] = r\"$[{\\rm M}/{\\rm H}]$\"\n",
    "label_latex['LOGG'] = r\"$\\log g$\"\n",
    "label_latex['TEFF'] = r\"$T_{\\rm eff}$\"\n",
    "label_latex['AK_WISE'] = r\"$A_K$\"\n",
    "\n",
    "label_y = np.hstack([np.array(x)[:, None] for x in label_ys.values()])\n",
    "label_err = np.hstack([np.array(x)[:, None] for x in label_errs.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99758742",
   "metadata": {},
   "source": [
    "# Make training and validation samples\n",
    "\n",
    "cut into eighths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5941dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "rando = rng.integers(8, size=len(f_all))\n",
    "train = rando != 0\n",
    "valid = (\n",
    "    ~train #&\n",
    "#     (g.LOGG < 2.2) &\n",
    "#     ((label_ys[label_name] * np.sqrt(label_weights[label_name])) > 4)\n",
    ")\n",
    "\n",
    "f_train = f_all[train]\n",
    "f_valid = f_all[valid]\n",
    "\n",
    "X_train, X_valid = f_train.X, f_valid.X\n",
    "y_train, y_valid = label_y[train], label_y[valid]\n",
    "w_train, w_valid = label_err[train], label_err[valid]\n",
    "\n",
    "print(X_train.shape, X_valid.shape)\n",
    "print(y_train.shape, y_valid.shape)\n",
    "print(w_train.shape, w_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0c95",
   "metadata": {},
   "source": [
    "# Define the LLVM model\n",
    "\n",
    "- This code more-or-less assumes that your data and labels and `B` are\n",
    "  normalized to reasonable ranges.\n",
    "- In the long run, `X` and `Y` don't need to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d82e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ParameterState:\n",
    "    sizes: dict\n",
    "    A: ('R', 'D')  # i.e. shape = (R, D)\n",
    "    B: ('Q', 'D')\n",
    "    z: ('N', 'D')\n",
    "    mu_X: ('R', )\n",
    "    mu_y: ('Q', )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        for name, field in self.__dataclass_fields__.items():\n",
    "            if name == 'sizes':\n",
    "                continue\n",
    "            shape = tuple([self.sizes[x] for x in field.type])\n",
    "            # setattr(self, name, np.array(getattr(self, name)))\n",
    "            \n",
    "            got_shape = getattr(self, name).shape\n",
    "            if got_shape != shape:\n",
    "                raise ValueError(\n",
    "                    f'Invalid shape for {name}: expected {field.type}={shape}, got {got_shape}'\n",
    "                )\n",
    "        \n",
    "    @property\n",
    "    def names(self):\n",
    "        return [x for x in self.__dataclass_fields__.keys() if x != 'sizes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86404e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _model_linear(mu, A, x):\n",
    "    # Formerly:\n",
    "    # pars.mu_x[None, :] + pars.z @ pars.A.T\n",
    "    # pars.mu_y[None, :] + pars.z @ pars.B.T\n",
    "    return mu[None] + x @ A.T\n",
    "\n",
    "\n",
    "class LinearLVM:\n",
    "    \n",
    "    def __init__(self, X, y, X_err, y_err, B, alpha, verbose=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like\n",
    "            shape `(N, R)` array of training features\n",
    "        y : array-like\n",
    "            shape `(N, Q)` array of training labels\n",
    "        X_err : array-like\n",
    "            shape `(N, R)` array of errors (standard deviations) for the features\n",
    "        y_err : array-like\n",
    "            shape `(N, Q)` array of errors (standard deviations) for the labels\n",
    "        B : array-like\n",
    "            shape `(Q, D)` matrix translating latents to labels.\n",
    "        alpha : numeric\n",
    "            regularization strength; use the source, Luke.\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.X = jnp.array(X)\n",
    "        self.y = jnp.array(y)\n",
    "        self.X_err = jnp.array(X_err)\n",
    "        self.y_err = jnp.array(y_err)\n",
    "        \n",
    "        self.sizes = {}\n",
    "        self.sizes['N'], self.sizes['R'] = self.X.shape    \n",
    "        self.sizes['Q'] = self.y.shape[1]\n",
    "        \n",
    "        shp_msg = \"Invalid shape for {object_name}: got {got}, expected {expected})\"\n",
    "        if self.y.shape[0] != self.sizes['N']:\n",
    "            shp_msg.format(\n",
    "                object_name=\"training labels y\",\n",
    "                got=self.y.shape[0],\n",
    "                expected=self.sizes['N']\n",
    "            )\n",
    "        if self.X_err.shape != self.X.shape:\n",
    "            shp_msg.format(\n",
    "                object_name=\"X_err\",\n",
    "                got=self.X_err.shape,\n",
    "                expected=self.X.shape\n",
    "            )\n",
    "        if self.y_err.shape != self.y.shape:\n",
    "            shp_msg.format(\n",
    "                object_name=\"y_err\",\n",
    "                got=self.y_err.shape,\n",
    "                expected=self.y.shape\n",
    "            )\n",
    "        \n",
    "        self._X_ivar = 1 / self.X_err**2\n",
    "        self._y_ivar = 1 / self.y_err**2\n",
    "        \n",
    "        # B turned into a Jax array below\n",
    "        B = np.array(B, copy=True)\n",
    "        _, self.sizes['D'] = B.shape\n",
    "        if B.shape[0] != self.sizes['Q']:\n",
    "            shp_msg.format(\n",
    "                object_name=\"B\",\n",
    "                got=B.shape[0],\n",
    "                expected=self.sizes['Q']\n",
    "            )\n",
    "            \n",
    "        # The elements of B that we will fit for should be set to nan in the \n",
    "        # input B array\n",
    "        self._B_fit_mask = jnp.isnan(B)\n",
    "        if not np.any(self._B_fit_mask) and verbose:\n",
    "            print(\"no free elements of B\")\n",
    "        elif np.any(self._B_fit_mask):\n",
    "            B[self._B_fit_mask] = 0.\n",
    "            if verbose:\n",
    "                print(f\"using {self._B_fit_mask.sum()} free elements of B\")\n",
    "        self.B = jnp.array(B)\n",
    "        if verbose:\n",
    "            print(f\"B = {B}\")\n",
    "            print(f\"B fit elements = {self._B_fit_mask}\")\n",
    "        \n",
    "        # Now assess which latents to fit:\n",
    "        self._z_fit_mask = jnp.any(self._B_fit_mask, axis=0)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"using {self._z_fit_mask.sum()} unconstrained elements of z, \"\n",
    "                f\"out of {self.sizes['D']} latents\"\n",
    "            )\n",
    "        \n",
    "        self.alpha = float(alpha)\n",
    "        \n",
    "        # Regularization matrix:\n",
    "        self.Lambda = self.alpha * np.diag(self._z_fit_mask.astype(int))\n",
    "        if verbose:\n",
    "            print(f\"Lambda = {self.Lambda}\")\n",
    "        assert self.alpha > 0., \"You must regularize, and strictly positively.\"\n",
    "        \n",
    "        # TODO:\n",
    "        self.par_state = self.initialize_par_state()\n",
    "        \n",
    "    def initialize_par_state(self, **state):\n",
    "        \"\"\"\n",
    "        TODO: this is a little hacky\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the means using invvar weighted means\n",
    "        # TODO: could do sigma-clipping here to be more robust\n",
    "        if 'mu_X' not in state:\n",
    "            state['mu_X'] = (\n",
    "                jnp.sum(self.X * self._X_ivar, axis=0) / \n",
    "                jnp.sum(self._X_ivar, axis=0)\n",
    "            )\n",
    "        \n",
    "        if 'mu_y' not in state:\n",
    "            state['mu_y'] = (\n",
    "                jnp.sum(self.y * self._y_ivar, axis=0) / \n",
    "                jnp.sum(self._y_ivar, axis=0)\n",
    "            )\n",
    "\n",
    "        if 'z' not in state:\n",
    "            # First hack: Start with the pseudo-inverse of `B`.\n",
    "            # BUG: Doesn't use weights.\n",
    "            state['z'] = jnp.linalg.lstsq(\n",
    "                self.B, \n",
    "                (self.y - state['mu_y'][None, :]).T, \n",
    "                rcond=None\n",
    "            )[0].T\n",
    "\n",
    "            # Second hack: Add noise.\n",
    "            # TODO: magic numberz\n",
    "            TINY = 1e-1 # MAGIC\n",
    "            sigma = np.std(state['z']) + TINY\n",
    "            state['z'] += np.random.normal(0, TINY * sigma, size=state['z'].shape)\n",
    "        \n",
    "        if 'A' not in state:\n",
    "            A = np.zeros((self.sizes['R'], self.sizes['D']))\n",
    "            state['A'] = A.copy()\n",
    "            for j, chi in enumerate(self._chi_X(state['mu_X'], state['A'], state['z']).T):\n",
    "                state['A'][j] = np.linalg.lstsq(\n",
    "                    state['z'] * self._X_ivar[:, j][None].T,\n",
    "                    chi, rcond=None\n",
    "                )[0]\n",
    "            \n",
    "        if 'B' not in state:\n",
    "            # TODO: implement this\n",
    "            state['B'] = self.B\n",
    "        \n",
    "        return ParameterState(sizes=self.sizes, **state)\n",
    "\n",
    "    def _chi_X(self, mu_X, A, z):\n",
    "        return (self.X - _model_linear(mu_X, A, z)) / self.X_err\n",
    "\n",
    "    def _chi_y(self, mu_y, B, z):\n",
    "        return (self.y - _model_linear(mu_y, B, z)) / self.y_err\n",
    "    \n",
    "    def unpack_p(self, p):\n",
    "        \"\"\"\n",
    "        TODO: deal with some of B is frozen\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        state = {}\n",
    "        for name in self.par_state.names:\n",
    "            if name == 'B':\n",
    "                # TODO: see note above\n",
    "                state['B'] = self.par_state.B\n",
    "                continue\n",
    "                \n",
    "            val = getattr(self.par_state, name)\n",
    "            state[name] = p[i:i+val.size].reshape(val.shape)\n",
    "            i += val.size\n",
    "        return ParameterState(sizes=self.sizes, **state)\n",
    "    \n",
    "    def pack_p(self, par_state=None):\n",
    "        \"\"\"\n",
    "        TODO: deal with some of B is frozen\n",
    "        \"\"\"\n",
    "        if par_state is None:\n",
    "            par_state = self.par_state\n",
    "            \n",
    "        arrs = []\n",
    "        for name in par_state.names:\n",
    "            if name == 'B':\n",
    "                # TODO: deal with note above\n",
    "                continue\n",
    "            val = getattr(par_state, name).flatten()\n",
    "            arrs.append(val)\n",
    "        return jnp.concatenate(arrs)\n",
    "    \n",
    "    def cost(self, p):\n",
    "        \"\"\"\n",
    "        TODO: Regularization term is totally wrong.\n",
    "        \"\"\"\n",
    "        pars = self.unpack_p(p)\n",
    "        # TODO: set par_state??\n",
    "        \n",
    "        chi_X = self._chi_X(pars.mu_X, pars.A, pars.z)\n",
    "        chi_y = self._chi_y(pars.mu_y, pars.B, pars.z)\n",
    "\n",
    "        return (\n",
    "            0.5 * jnp.sum(chi_X ** 2) +\n",
    "            0.5 * jnp.sum(chi_y ** 2) +\n",
    "            0.5 * self.alpha * jnp.sum(pars.z[:, self._z_fit_mask] ** 2)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, p):\n",
    "        val = self.cost(p)\n",
    "        return val\n",
    "    \n",
    "    def predict_y(self, X, X_err, par_state=None):\n",
    "        if par_state is None:\n",
    "            par_state = self.par_state\n",
    "            \n",
    "        # should this use the regularization matrix? Hogg thinks not.\n",
    "        M = X.shape[0]\n",
    "        if X.shape[1] != self.sizes['R']:\n",
    "            raise ValueError(\"Invalid shape for input feature matrix X\")\n",
    "            \n",
    "        y_hat = np.zeros((M, self.sizes['Q']))\n",
    "        \n",
    "        chi = (X - par_state.mu_X[None, :]) / X_err\n",
    "        for i, dx in enumerate(chi):\n",
    "            M = par_state.A / X_err[i][:, None]\n",
    "            z = np.linalg.lstsq(M, dx, rcond=None)[0]\n",
    "            y_hat[i] = par_state.mu_y + par_state.B @ z\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b006b31",
   "metadata": {},
   "source": [
    "Very dumb fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "Atrue = rng.normal(size=(17, 5))\n",
    "Btrue = np.zeros((3,5))\n",
    "Btrue[:3,:3] = np.eye(3)\n",
    "Ztrue = rng.normal(size=(191, 5))\n",
    "Zstartrue = rng.normal(size=(53, 5))\n",
    "Xtrue = Ztrue @ Atrue.T\n",
    "Xstartrue = Zstartrue @ Atrue.T\n",
    "sigma = 0.1\n",
    "X = Xtrue + sigma * rng.normal(size=Xtrue.shape)\n",
    "Xstar = Xstartrue + sigma * rng.normal(size=Xstartrue.shape)\n",
    "Ytrue = Ztrue @ Btrue.T\n",
    "Ystartrue = Zstartrue @ Btrue.T\n",
    "Y = Ytrue + sigma * rng.normal(size=Ytrue.shape)\n",
    "Ystar = Ystartrue + sigma * rng.normal(size=Ystartrue.shape)\n",
    "xerr = np.zeros_like(X) + sigma\n",
    "xstarerr = np.zeros_like(Xstar) + sigma\n",
    "yerr = np.zeros_like(Y) + sigma\n",
    "ystarerr = np.zeros_like(Ystar) + sigma\n",
    "\n",
    "B = np.zeros((3,7))\n",
    "B[:3, :3] = np.eye(3)\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75322fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llvm = LinearLVM(X, Y, xerr, yerr, B, alpha, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in llvm.par_state.names:\n",
    "#     assert np.all(getattr(llvm.unpack_p(llvm.pack_p()), name) == getattr(llvm.par_state, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "llvm_grad = jax.grad(llvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609736fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = llvm.pack_p()\n",
    "\n",
    "# GLOBALS: careful!!\n",
    "ks = [0]\n",
    "vals = [llvm(x0)]\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.plot(ks, vals)\n",
    "\n",
    "def callback(x):\n",
    "    global ks, vals, ax\n",
    "    ks.append(ks[-1] + 1)\n",
    "    vals.append(llvm(x))\n",
    "    if (ks[-1] % 4) == 0:\n",
    "        ax.cla()\n",
    "        ax.plot(ks, vals)\n",
    "    \n",
    "    if (ks[-1] % 10) == 0:\n",
    "        print(f\"{ks[-1]}\", end='\\r')\n",
    "    \n",
    "res = minimize(\n",
    "    llvm, \n",
    "    jac=llvm_grad, \n",
    "    x0=x0, \n",
    "    callback=callback, \n",
    "    options=dict(maxiter=512),\n",
    "    method='bfgs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks[-10:], vals[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_state = llvm.unpack_p(res.x)\n",
    "ystar_predict0 = llvm.predict_y(Xstar, xstarerr, llvm.par_state)\n",
    "ystar_predict = llvm.predict_y(Xstar, xstarerr, res_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer for test-set objects\n",
    "for k in range(Ystar.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.scatter(Ystar[:, k], ystar_predict0[:, k], c=\"r\", marker=\"o\")\n",
    "    plt.scatter(Ystar[:, k], ystar_predict[:, k], c=\"k\", marker=\"o\")\n",
    "    plt.plot([Ystar[:, k].min(), Ystar[:, k].max()],\n",
    "             [Ystar[:, k].min(), Ystar[:, k].max()], \n",
    "             marker='', color='tab:blue')\n",
    "    plt.xlabel(f\"true label {k}\")\n",
    "    plt.ylabel(f\"prediction of label {k}\")\n",
    "    plt.title(\"held-out data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d1c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8b5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrian conda base",
   "language": "python",
   "name": "conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
