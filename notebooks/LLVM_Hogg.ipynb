{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b6ea29",
   "metadata": {},
   "source": [
    "# Experiments with a linear latent-variable model\n",
    "\n",
    "## Authors:\n",
    "- **Adrian Price-Whelan** (Flatiron)\n",
    "- **David W. Hogg** (NYU) (MPIA) (Flatiron)\n",
    "\n",
    "## Hyper-parameters:\n",
    "- *add key hyper-parameters here.*\n",
    "\n",
    "## TODO / questions:\n",
    "- There is no reason that `X` and `Y` (training data) must contain the same number of objects, people.\n",
    "- Right now this only deals with weight vectors, not full covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53898a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7ae09",
   "metadata": {},
   "source": [
    "## Make LLVM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class llvm:\n",
    "    def __init__(self, X, Y, Wx, Wy, B, Lambda):\n",
    "        \"\"\"\n",
    "        ## inputs:\n",
    "        `X`: shape `(n, r)` block of training features\n",
    "        `Y`: shape `(n, q)` block of training labels\n",
    "        `Wx`: shape `(n, r)` block of inverse variances (weights) for the features\n",
    "        `Wy`: shape `(n, q)` block of inverse variances (weights) for the labels\n",
    "        `B`: shape `(q, d)` matrix translating latents to labels.\n",
    "        `Lambda`: regularization strength; use the source, Luke.\n",
    "        \n",
    "        ## bugs / comments:\n",
    "        - Put `np.NaN` into the parts of `B` you want to optimize!\n",
    "        - This code more-or-less assumes that your data and labels and `B` are\n",
    "          normalized to reasonable ranges.\n",
    "        - In the long run, `X` and `Y` don't need to be the same length.\n",
    "        \"\"\"\n",
    "        self.X = jnp.array(X)\n",
    "        self.Y = jnp.array(Y)\n",
    "        self.n, self.r = self.X.shape\n",
    "        enn, self.q = self.Y.shape\n",
    "        assert enn == self.n, \"llvm: Right now, the training data must be rectangular.\"\n",
    "        self.Wx = jnp.array(Wx)\n",
    "        assert self.Wx.shape == self.X.shape, \"llvm: Inconsistency between `X` and `Wx`.\"\n",
    "        assert np.all(self.Wx >= 0.), \"llvm: Weights can't be negative.\"\n",
    "        self.sqrtWx = jnp.sqrt(self.Wx)\n",
    "        self.Wy = jnp.array(Wy)\n",
    "        assert self.Wy.shape == self.Y.shape, \"llvm: Inconsistency between `Y` and `Wy`.\"\n",
    "        assert np.all(self.Wy >= 0.), \"llvm: Weights can't be negative.\"\n",
    "        self.sqrtWy = jnp.sqrt(Wy)\n",
    "        self.B = jnp.array(B)\n",
    "        cue, self.d = B.shape\n",
    "        assert cue == self.q, \"llvm: Inconsistency between `B` and `Y`.\"\n",
    "        self.B_elements_to_fit = jnp.isnan(self.B)\n",
    "        if jnp.sum(self.B_elements_to_fit) == 0:\n",
    "            self.B_elements_to_fit = None\n",
    "            print(\"llvm: found no free elements of `B`.\")\n",
    "        else:\n",
    "            self.B[self.B_elements_to_fit] = 0.\n",
    "            print(\"llvm: found\", np.sum(self.B_elements_to_fit),\n",
    "                  \"free elements of `B`.\")\n",
    "        self.free_elements_of_Z = jnp.isclose(jnp.sum(self.B, axis=0), 0.)\n",
    "        print(self.free_elements_of_Z)\n",
    "        print(\"llvm: found\", np.sum(self.free_elements_of_Z),\n",
    "              \"unconstrained elements of `Z`, of\", self.d)\n",
    "        self.Lambda = Lambda\n",
    "        self.regularization_matrix = Lambda * np.diag(self.free_elements_of_Z.astype(int))\n",
    "        print(self.regularization_matrix)\n",
    "        assert Lambda > 0., \"llvm: You must regularize, and strictly positively.\"\n",
    "        self.initialize_latents()\n",
    "        return\n",
    "\n",
    "    def _predict_X(self):\n",
    "        return self.mux[None, :] + self.Z @ self.A.T\n",
    "\n",
    "    def _predict_Y(self):\n",
    "        return self.muy[None, :] + self.Z @ self.B.T\n",
    "\n",
    "    def _chi_X(self):\n",
    "        return (self.X - self._predict_X()) * self.sqrtWx\n",
    "\n",
    "    def _chi_Y(self):\n",
    "        return (self.Y - self._predict_Y()) * self.sqrtWy\n",
    "\n",
    "    def _cost(self):\n",
    "        \"\"\"\n",
    "        WARNING: Regularization term is totally wrong.\n",
    "        \"\"\"\n",
    "        Xchi, Ychi = self._chi_X(), self._chi_Y()\n",
    "        return 0.5 * jnp.sum(self._chi_X() ** 2) \\\n",
    "             + 0.5 * jnp.sum(self._chi_Y() ** 2) \\\n",
    "             + 0.5 * self.Lambda * jnp.sum(self.Z[:, self.free_elements_of_Z] ** 2)\n",
    "\n",
    "    def predict_y_given_x(self, Xstar, Wxstar):\n",
    "        # should this use the regularization matrix? Hogg thinks not.\n",
    "        m, arrh = Xstar.shape\n",
    "        assert arrh == self.r\n",
    "        Ystarhat = np.zeros((m, self.q))\n",
    "        sqrtWx = np.sqrt(Wxstar)\n",
    "        chi = (Xstar - self.mux[None, :]) * sqrtWx\n",
    "        for i, x in enumerate(chi):\n",
    "            M = self.A * sqrtWx[i][:, None]\n",
    "            z = np.linalg.lstsq(M, x, rcond=None)[0]\n",
    "            Ystarhat[i] = self.muy + self.B @ z\n",
    "        return Ystarhat\n",
    "\n",
    "    def initialize_latents(self):\n",
    "        # this is a bag of hacks.\n",
    "\n",
    "        # Zeoth hack: Take means.\n",
    "        self.mux = jnp.sum(self.X * self.Wx, axis=0) / jnp.sum(self.Wx, axis=0)\n",
    "        assert self.mux.shape == (self.r, )\n",
    "        self.muy = jnp.sum(self.Y * self.Wy, axis=0) / jnp.sum(self.Wy, axis=0)\n",
    "        assert self.muy.shape == (self.q, )\n",
    "\n",
    "        # First hack: Start with the pseudo-inverse of `B`.\n",
    "        # BUG: Doesn't use weights.\n",
    "        self.Z = jnp.linalg.lstsq(self.B, (self.Y - self.muy[None, :]).T, rcond=None)[0].T\n",
    "        assert self.Z.shape == (self.n, self.d)\n",
    "\n",
    "        # Second hack: Add noise.\n",
    "        TINY = 1.e-1 # MAGIC\n",
    "        sigma = np.std(self.Z) + TINY\n",
    "        self.Z += 0.1 * sigma * np.random.normal(size=self.Z.shape) # MAGIC\n",
    "\n",
    "        # Third hack: Run A and B steps.\n",
    "        self.A = jnp.zeros((self.r, self.d))\n",
    "        self.A_step()\n",
    "        self.B_step()\n",
    "        \n",
    "    def optimize_step(self, ftol=0.1):\n",
    "        self._renorm_A()\n",
    "        before = self._cost()\n",
    "        self.Z_step()\n",
    "        self.mu_step()\n",
    "        self.A_step()\n",
    "        self.B_step()\n",
    "        after = self._cost()\n",
    "        print(\"optimize_step(): Cost after:\", after, self.n * (self.r + self.q))\n",
    "        return after < (before - ftol)\n",
    "\n",
    "    def Z_step(self):\n",
    "        # BUG: DOESN'T DO REGULARIZATION RIGHT.\n",
    "        dZ = np.zeros_like(self.Z)\n",
    "        for i, (x, y) in enumerate(zip(self._chi_X(),\n",
    "                                       self._chi_Y())):\n",
    "            resid = np.append(x, y)\n",
    "            matrix = np.concatenate((self.A * (self.sqrtWx[i])[:, None],\n",
    "                                     self.B * (self.sqrtWy[i])[:, None]),\n",
    "                                    axis=0)\n",
    "            dZ[i] = np.linalg.lstsq(matrix.T @ matrix\n",
    "                                    + self.regularization_matrix,\n",
    "                                    matrix.T @ resid, rcond=None)[0]\n",
    "        self.Z = self.Z + dZ\n",
    "        return\n",
    "\n",
    "    def A_step(self):\n",
    "        dA = np.zeros_like(self.A)\n",
    "        for j, x in enumerate(self._chi_X().T):\n",
    "            dA[j] = np.linalg.lstsq(self.Z * self.sqrtWx[:, j][None, :].T,\n",
    "                                    x, rcond=None)[0]\n",
    "        self.A = self.A + dA\n",
    "        return\n",
    "\n",
    "    def _renorm_A(self):\n",
    "        renorm = np.sqrt(np.sum(self.A[:, self.free_elements_of_Z] ** 2, axis=0))\n",
    "        self.A.at[:, self.free_elements_of_Z].divide(renorm[None, :])\n",
    "        self.Z.at[:, self.free_elements_of_Z].multiply(renorm[None, :])\n",
    "        return\n",
    "\n",
    "    def B_step(self):\n",
    "        # BUG: Not currently operable.\n",
    "        if self.B_elements_to_fit is None:\n",
    "            return\n",
    "        assert False\n",
    "        return\n",
    "\n",
    "    def mu_step(self):\n",
    "        Xresid = self.X - self._predict_X()\n",
    "        self.mux = self.mux + jnp.sum(Xresid * self.Wx, axis=0) / jnp.sum(self.Wx, axis=0)\n",
    "        Yresid = self.Y - self._predict_Y()\n",
    "        self.muy = self.muy + jnp.sum(Yresid * self.Wy, axis=0) / jnp.sum(self.Wy, axis=0)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd057c1",
   "metadata": {},
   "source": [
    "## Make fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c21635",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17) # whoops\n",
    "Atrue = np.random.normal(size=(17, 5))\n",
    "Btrue = np.zeros((3,5))\n",
    "Btrue[:3,:3] = np.eye(3)\n",
    "Ztrue = np.random.normal(size=(191, 5))\n",
    "Zstartrue = np.random.normal(size=(53, 5))\n",
    "Xtrue = Ztrue @ Atrue.T\n",
    "Xstartrue = Zstartrue @ Atrue.T\n",
    "sigma = 0.1\n",
    "X = Xtrue + sigma * np.random.normal(size=Xtrue.shape)\n",
    "Xstar = Xstartrue + sigma * np.random.normal(size=Xstartrue.shape)\n",
    "Ytrue = Ztrue @ Btrue.T\n",
    "Ystartrue = Zstartrue @ Btrue.T\n",
    "Y = Ytrue + sigma * np.random.normal(size=Ytrue.shape)\n",
    "Ystar = Ystartrue + sigma * np.random.normal(size=Ystartrue.shape)\n",
    "Wx = np.zeros_like(X) + sigma ** -2\n",
    "Wxstar = np.zeros_like(Xstar) + sigma ** -2\n",
    "Wy = np.zeros_like(Y) + sigma ** -2\n",
    "Wystar = np.zeros_like(Ystar) + sigma ** -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa142ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.zeros((3,7))\n",
    "B[:3, :3] = np.eye(3)\n",
    "Lambda = 0.1\n",
    "LLVM = llvm(X, Y, Wx, Wy, B, Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while LLVM.optimize_step():\n",
    "    True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LLVM.mux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show self-test\n",
    "plt.scatter(Y[:, 0], LLVM._predict_Y()[:, 0], c=\"k\", marker=\"o\")\n",
    "plt.xlabel(\"true label 0\")\n",
    "plt.ylabel(\"prediction of label 0\")\n",
    "plt.title(\"self test\")\n",
    "f = plt.figure()\n",
    "plt.scatter(X[:, 0], LLVM._predict_X()[:, 0], c=\"k\", marker=\"o\")\n",
    "plt.xlabel(\"true feature 0\")\n",
    "plt.ylabel(\"prediction of feature 0\")\n",
    "plt.title(\"self test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e510ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer for test-set objects\n",
    "Yhat = LLVM.predict_y_given_x(Xstar, Wxstar)\n",
    "plt.scatter(Ystar[:, 0], Yhat[:, 0], c=\"k\", marker=\"o\")\n",
    "plt.xlabel(\"true label 0\")\n",
    "plt.ylabel(\"prediction of label 0\")\n",
    "plt.title(\"held-out data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0ac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
